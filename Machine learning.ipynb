{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ebe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d076b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:04.446921Z",
     "iopub.status.busy": "2025-08-13T23:05:04.446521Z",
     "iopub.status.idle": "2025-08-13T23:05:32.877264Z",
     "shell.execute_reply": "2025-08-13T23:05:32.876334Z"
    },
    "papermill": {
     "duration": 28.440411,
     "end_time": "2025-08-13T23:05:32.879051",
     "exception": false,
     "start_time": "2025-08-13T23:05:04.438640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ==============================\n",
    "# üîÆ Deep Learning (TensorFlow / Keras)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa534166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:32.891364Z",
     "iopub.status.busy": "2025-08-13T23:05:32.890582Z",
     "iopub.status.idle": "2025-08-13T23:05:33.167399Z",
     "shell.execute_reply": "2025-08-13T23:05:33.166514Z"
    },
    "papermill": {
     "duration": 0.284583,
     "end_time": "2025-08-13T23:05:33.169204",
     "exception": false,
     "start_time": "2025-08-13T23:05:32.884621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('retail_store_inventory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec4b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:33.181146Z",
     "iopub.status.busy": "2025-08-13T23:05:33.180469Z",
     "iopub.status.idle": "2025-08-13T23:05:33.213217Z",
     "shell.execute_reply": "2025-08-13T23:05:33.212135Z"
    },
    "papermill": {
     "duration": 0.040723,
     "end_time": "2025-08-13T23:05:33.215216",
     "exception": false,
     "start_time": "2025-08-13T23:05:33.174493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q1 = df['Units Sold'].quantile(0.25)\n",
    "Q3 = df['Units Sold'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + (1.5 * IQR)\n",
    "print(f\"25th Percentile (Q1): {Q1}\")\n",
    "print(f\"75th Percentile (Q3): {Q3}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR}\")\n",
    "print(f\"Upper Bound for Outliers: {upper_bound:.2f}\")\n",
    "outliers_count_before = df[df['Units Sold'] > upper_bound].shape[0]\n",
    "print(f\"Found {outliers_count_before} records with 'Units Sold' above the upper bound.\")\n",
    "df['Units Sold'] = np.where(\n",
    "    df['Units Sold'] > upper_bound,\n",
    "    upper_bound,\n",
    "    df['Units Sold']\n",
    ")\n",
    "print(f\"‚úÖ Successfully capped {outliers_count_before} outliers directly within the 'Units Sold' column.\")\n",
    "print(\"\\nDescription of the 'Units Sold' column AFTER capping:\")\n",
    "print(df['Units Sold'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa601599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:33.227427Z",
     "iopub.status.busy": "2025-08-13T23:05:33.227066Z",
     "iopub.status.idle": "2025-08-13T23:05:34.146382Z",
     "shell.execute_reply": "2025-08-13T23:05:34.145375Z"
    },
    "papermill": {
     "duration": 0.92747,
     "end_time": "2025-08-13T23:05:34.148084",
     "exception": false,
     "start_time": "2025-08-13T23:05:33.220614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Store ID', 'Product ID', 'Date']).reset_index(drop=True)\n",
    "print(\"\\n--- Starting Comprehensive Feature Engineering ---\")\n",
    "print(\"Step 1: Creating date-based features...\")\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['quarter'] = df['Date'].dt.quarter\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "print(\"Step 2: Creating cyclical (sin/cos) features...\")\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_month_sin'] = np.sin(2 * np.pi * df['day_of_month'] / 30.5)\n",
    "df['day_of_month_cos'] = np.cos(2 * np.pi * df['day_of_month'] / 30.5)\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "print(\"Step 3: Creating lag features...\")\n",
    "grouped = df.groupby(['Store ID', 'Product ID'])\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 14, 21, 30, 31, 60, 90, 365]\n",
    "for lag in lags:\n",
    "    df[f'sales_lag_{lag}'] = grouped['Units Sold'].shift(lag)\n",
    "print(\"Step 4: Creating rolling window features...\")\n",
    "windows = [3, 7, 14, 30, 60]\n",
    "shifted_sales = grouped['Units Sold'].shift(1)\n",
    "for window in windows:\n",
    "    df[f'sales_rolling_mean_{window}'] = shifted_sales.rolling(window=window).mean()\n",
    "    df[f'sales_rolling_std_{window}'] = shifted_sales.rolling(window=window).std()\n",
    "    df[f'sales_rolling_min_{window}'] = shifted_sales.rolling(window).min()\n",
    "    df[f'sales_rolling_median_{window}'] = shifted_sales.rolling(window).median()\n",
    "    df[f'sales_rolling_max_{window}'] = shifted_sales.rolling(window).max()\n",
    "df['sales_expanding_mean'] = grouped['Units Sold'].shift(1).expanding().mean()\n",
    "print(\"Step 5: Creating advanced interaction and ratio-based features...\")\n",
    "df['price_x_discount'] = df['Price'] * (1 - df['Discount'] / 100.0)\n",
    "df['holiday_x_weekend'] = df['Holiday/Promotion'] * df['is_weekend']\n",
    "df['price_rolling_std_7'] = grouped['Price'].shift(1).rolling(window=7).std()\n",
    "df['lag_momentum_7_1'] = df['sales_lag_1'] - df['sales_lag_7']\n",
    "df['lag_momentum_30_7'] = df['sales_lag_7'] - df['sales_lag_30']\n",
    "df['rolling_mean_ratio_7_30'] = df['sales_rolling_mean_7'] / (df['sales_rolling_mean_30'] + 1)\n",
    "df['volatility_trend_7d'] = (df['sales_lag_1'] - df['sales_rolling_mean_7']) / (df['sales_rolling_std_7'] + 1)\n",
    "print(\"Step 6: Creating domain-specific retail 'super features'...\")\n",
    "df['sell_through_rate_7d'] = df['sales_rolling_mean_7'] / (df['sales_rolling_mean_7'] + df['Inventory Level'] + 1e-6)\n",
    "df['days_of_supply_7d'] = df['Inventory Level'] / (df['sales_rolling_mean_7'] + 1e-6)\n",
    "df['price_elasticity_proxy'] = df['sales_rolling_mean_30'] / (df['price_x_discount'].clip(lower=0.01))\n",
    "df['product_day_avg'] = df.groupby(['Product ID', 'day_of_week'])['Units Sold'].transform('mean')\n",
    "df['store_day_avg'] = df.groupby(['Store ID', 'day_of_week'])['Units Sold'].transform('mean')\n",
    "df['product_month_avg'] = df.groupby(['Product ID', 'month'])['Units Sold'].transform('mean')\n",
    "is_promo = df['Holiday/Promotion'] == 1\n",
    "df['promo_date'] = df['Date'].where(is_promo)\n",
    "df['last_promo_date'] = grouped['promo_date'].ffill()\n",
    "df['days_since_promo'] = (df['Date'] - df['last_promo_date']).dt.days\n",
    "df['promo_momentum_7d'] = grouped['Holiday/Promotion'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).sum()\n",
    ")\n",
    "df = df.drop(columns=['promo_date', 'last_promo_date'])\n",
    "df['avg_price_30d'] = grouped['Price'].shift(1).rolling(30, min_periods=1).mean()\n",
    "df['price_vs_avg']  = df['Price'] / (df['avg_price_30d'] + 1e-6)\n",
    "df['promo_intensity_30d'] = grouped['Holiday/Promotion'].transform(\n",
    "    lambda x: x.rolling(30, min_periods=1).sum()\n",
    ")\n",
    "df['avg_discount_30d'] = grouped['Discount'].shift(1).rolling(30, min_periods=1).mean()\n",
    "df['discount_spread']   = df['Discount'] - df['avg_discount_30d']\n",
    "for w in [7, 30]:\n",
    "    df[f'sales_rolling_median_{w}'] = shifted_sales.rolling(window=w).median()\n",
    "df['sales_ewm_14d'] = shifted_sales.ewm(span=14, adjust=False).mean()\n",
    "df['is_month_start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end']   = df['Date'].dt.is_month_end.astype(int)\n",
    "df['category_day_avg']  = df.groupby(['Category','Date'])['Units Sold'].transform('mean')\n",
    "network_avg = df.groupby('Date')['Units Sold'].transform('mean')\n",
    "df['store_vs_network']  = df['Units Sold'] - network_avg\n",
    "df['days_left_in_month'] = df['Date'].dt.days_in_month - df['day_of_month']\n",
    "df['days_left_in_quarter'] = ((df['quarter'] * 3) - df['month']) * 30 + df['days_left_in_month']\n",
    "df['sales_diff_1_2'] = df['sales_lag_1'] - df['sales_lag_2']\n",
    "df['sales_diff_7_14'] = df['sales_lag_7'] - df['sales_lag_14']\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df_processed = df.copy()\n",
    "print(\"\\n‚úÖ Comprehensive feature engineering complete.\")\n",
    "print(f\"Final shape of processed data before splitting: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83cf01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:34.159870Z",
     "iopub.status.busy": "2025-08-13T23:05:34.159552Z",
     "iopub.status.idle": "2025-08-13T23:05:34.241156Z",
     "shell.execute_reply": "2025-08-13T23:05:34.240164Z"
    },
    "papermill": {
     "duration": 0.089135,
     "end_time": "2025-08-13T23:05:34.242627",
     "exception": false,
     "start_time": "2025-08-13T23:05:34.153492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "low_cardinality_cols = ['Region', 'Weather Condition', 'Seasonality']\n",
    "print(f\"Applying one-hot encoding to low-cardinality columns: {low_cardinality_cols}\")\n",
    "df_processed = pd.get_dummies(df_processed, columns=low_cardinality_cols, drop_first=True)\n",
    "high_cardinality_cols = ['Store ID', 'Product ID', 'Category']\n",
    "print(f\"Converting high-cardinality columns to 'category' dtype for efficient model handling: {high_cardinality_cols}\")\n",
    "for col in high_cardinality_cols:\n",
    "    df_processed[col] = df_processed[col].astype('category')\n",
    "df_processed = df_processed.sort_values('Date').reset_index(drop=True)\n",
    "print(\"\\n‚úÖ Categorical feature handling complete.\")\n",
    "print(f\"Final shape of processed data before splitting: {df_processed.shape}\")\n",
    "print(\"\\nData types of key categorical columns:\")\n",
    "print(df_processed[['Store ID', 'Product ID', 'Category']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409fbecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:34.254728Z",
     "iopub.status.busy": "2025-08-13T23:05:34.254105Z",
     "iopub.status.idle": "2025-08-13T23:05:38.269858Z",
     "shell.execute_reply": "2025-08-13T23:05:38.268976Z"
    },
    "papermill": {
     "duration": 4.023804,
     "end_time": "2025-08-13T23:05:38.271678",
     "exception": false,
     "start_time": "2025-08-13T23:05:34.247874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_drop_for_analysis = [\n",
    "    'Units Sold',\n",
    "    'Demand Forecast',\n",
    "    'Date',\n",
    "    'Competitor Pricing',\n",
    "    'Units Ordered',\n",
    "]\n",
    "X_analysis = df_processed.drop(columns=features_to_drop_for_analysis)\n",
    "y_analysis = df_processed['Units Sold']\n",
    "categorical_cols_analysis = X_analysis.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"One-hot encoding for analysis: {list(categorical_cols_analysis)}\")\n",
    "X_analysis_encoded = pd.get_dummies(X_analysis, columns=categorical_cols_analysis, drop_first=True)\n",
    "print(\"\\nTraining a discovery model to find feature importances...\")\n",
    "discovery_model = lgb.LGBMRegressor(random_state=42, n_estimators=200)\n",
    "discovery_model.fit(X_analysis_encoded, y_analysis)\n",
    "print(\"Discovery model training complete.\")\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_analysis_encoded.columns,\n",
    "    'importance': discovery_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "N_TOP_FEATURES = 20\n",
    "print(f\"\\n--- Top {N_TOP_FEATURES} Most Important Features ---\")\n",
    "print(feature_importances.head(N_TOP_FEATURES))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances.head(N_TOP_FEATURES))\n",
    "plt.title(f'Top {N_TOP_FEATURES} Feature Importances for Predicting Units Sold', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "NUM_FEATURES_TO_KEEP = 12\n",
    "top_features = feature_importances['feature'].head(NUM_FEATURES_TO_KEEP).tolist()\n",
    "print(f\"\\n--- Selecting the Top {NUM_FEATURES_TO_KEEP} Features for Model Training ---\")\n",
    "print(top_features)\n",
    "X = X_analysis_encoded[top_features]\n",
    "y = y_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fe75e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:38.289245Z",
     "iopub.status.busy": "2025-08-13T23:05:38.288556Z",
     "iopub.status.idle": "2025-08-13T23:05:38.299698Z",
     "shell.execute_reply": "2025-08-13T23:05:38.298792Z"
    },
    "papermill": {
     "duration": 0.021318,
     "end_time": "2025-08-13T23:05:38.301069",
     "exception": false,
     "start_time": "2025-08-13T23:05:38.279751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_point = int(len(df_processed) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "train_dates = df_processed.loc[X_train.index, 'Date']\n",
    "test_dates = df_processed.loc[X_test.index, 'Date']\n",
    "print(f\"Training data from: {train_dates.min().date()} to {train_dates.max().date()}\")\n",
    "print(f\"Testing data from:  {test_dates.min().date()} to {test_dates.max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463876ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:05:38.318889Z",
     "iopub.status.busy": "2025-08-13T23:05:38.318580Z",
     "iopub.status.idle": "2025-08-13T23:06:51.322101Z",
     "shell.execute_reply": "2025-08-13T23:06:51.321263Z"
    },
    "papermill": {
     "duration": 73.015446,
     "end_time": "2025-08-13T23:06:51.324462",
     "exception": false,
     "start_time": "2025-08-13T23:05:38.309016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "standard_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(random_state=42),\n",
    "    \"Lasso Regression\": Lasso(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(random_state=42, verbose=0)\n",
    "}\n",
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate MAE, MSE, RMSE, NRMSE, R¬≤, MAPE, and additional percentage metrics for model evaluation\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    y_range = np.max(y_true) - np.min(y_true)\n",
    "    nrmse = rmse / y_range if y_range != 0 else 0\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    y_true_array = np.array(y_true)\n",
    "    y_pred_array = np.array(y_pred)\n",
    "    threshold = np.abs(y_true_array).mean() * 0.01\n",
    "    mask = np.abs(y_true_array) > threshold\n",
    "    if np.any(mask):\n",
    "        mape = np.mean(np.abs((y_true_array[mask] - y_pred_array[mask]) / y_true_array[mask])) * 100\n",
    "    else:\n",
    "        mape = float('inf')\n",
    "    mape_mean = (mae / np.abs(y_true_array).mean()) * 100 if np.abs(y_true_array).mean() != 0 else float('inf')\n",
    "    smape = np.mean(2 * np.abs(y_true_array - y_pred_array) / (np.abs(y_true_array) + np.abs(y_pred_array))) * 100\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'nrmse': nrmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'mape_mean': mape_mean,\n",
    "        'smape': smape\n",
    "    }\n",
    "model_results = {}\n",
    "print(\"üöÄ Training Standard Models...\")\n",
    "print(\"=\" * 90)\n",
    "for name, model in standard_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    metrics = calculate_all_metrics(y_test, predictions)\n",
    "    model_results[name] = {\n",
    "        **metrics,\n",
    "        'predictions': predictions, \n",
    "        'model_object': model\n",
    "    }\n",
    "    print(f\"‚úÖ {name}:\")\n",
    "    print(f\"   MAE: {metrics['mae']:.3f} | RMSE: {metrics['rmse']:.3f} | NRMSE: {metrics['nrmse']:.3f} | R¬≤: {metrics['r2']:.3f}\")\n",
    "    print(f\"   MAPE: {metrics['mape']:.2f}% | MAPE-Mean: {metrics['mape_mean']:.2f}% | SMAPE: {metrics['smape']:.2f}%\")\n",
    "    print()\n",
    "print(\"\\n--- Training LightGBM with Early Stopping ---\")\n",
    "lgbm = lgb.LGBMRegressor(random_state=42, n_estimators=1000, n_jobs=-1)\n",
    "lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='mae', callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "lgbm_predictions = lgbm.predict(X_test)\n",
    "lgbm_metrics = calculate_all_metrics(y_test, lgbm_predictions)\n",
    "model_results[\"LightGBM (Optimized)\"] = {\n",
    "    **lgbm_metrics,\n",
    "    'predictions': lgbm_predictions, \n",
    "    'model_object': lgbm\n",
    "}\n",
    "print(f\"‚úÖ LightGBM (Optimized):\")\n",
    "print(f\"   MAE: {lgbm_metrics['mae']:.3f} | RMSE: {lgbm_metrics['rmse']:.3f} | NRMSE: {lgbm_metrics['nrmse']:.3f} | R¬≤: {lgbm_metrics['r2']:.3f}\")\n",
    "print(f\"   MAPE: {lgbm_metrics['mape']:.2f}% | MAPE-Mean: {lgbm_metrics['mape_mean']:.2f}% | SMAPE: {lgbm_metrics['smape']:.2f}%\")\n",
    "print(\"\\n\\n--- Training Deep Learning Model (BiLSTM) ---\")\n",
    "scaler_X = MinMaxScaler(); X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_y = MinMaxScaler(); y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "def create_sequences(X, y, time_steps=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "TIME_STEPS = 30\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, TIME_STEPS)\n",
    "n_test = len(y_test)\n",
    "X_train_seq, X_test_seq = X_seq[:-n_test], X_seq[-n_test:]\n",
    "y_train_seq, y_test_seq = y_seq[:-n_test], y_seq[-n_test:]\n",
    "print(f\"Created {len(X_train_seq)} training sequences and {len(X_test_seq)} testing sequences.\")\n",
    "print(\"\\n\\n--- Creating Hybrid Ensemble Model ---\")\n",
    "print(\"=\" * 90)\n",
    "hybrid_preds = (\n",
    "    model_results['Lasso Regression']['predictions'] +\n",
    "    model_results['LightGBM (Optimized)']['predictions'] +\n",
    "    model_results['Random Forest']['predictions']\n",
    ") / 3\n",
    "hybrid_metrics = calculate_all_metrics(y_test, hybrid_preds)\n",
    "model_results[\"Hybrid Model (Lasso + LightGBM + RF)\"] = {\n",
    "    **hybrid_metrics,\n",
    "    'predictions': hybrid_preds,\n",
    "    'model_object': 'ensemble'\n",
    "}\n",
    "print(f\"üí° Hybrid Model (Lasso + LightGBM + RF):\")\n",
    "print(f\"   MAE: {hybrid_metrics['mae']:.3f} | RMSE: {hybrid_metrics['rmse']:.3f} | NRMSE: {hybrid_metrics['nrmse']:.3f} | R¬≤: {hybrid_metrics['r2']:.3f}\")\n",
    "print(f\"   MAPE: {hybrid_metrics['mape']:.2f}% | MAPE-Mean: {hybrid_metrics['mape_mean']:.2f}% | SMAPE: {hybrid_metrics['smape']:.2f}%\")\n",
    "print(\"\\n\\n--- Final Model Selection (based on MAE) ---\")\n",
    "print(\"=\" * 90)\n",
    "best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "best_metrics = model_results[best_model_name]\n",
    "print(f\"üèÜ ULTIMATE BEST MODEL: '{best_model_name}'\")\n",
    "print(f\"   üìä Performance Metrics:\")\n",
    "print(f\"      ‚Ä¢ MAE (Mean Absolute Error): {best_metrics['mae']:.3f}\")\n",
    "print(f\"      ‚Ä¢ MSE (Mean Squared Error): {best_metrics['mse']:.3f}\")\n",
    "print(f\"      ‚Ä¢ RMSE (Root Mean Squared Error): {best_metrics['rmse']:.3f}\")\n",
    "print(f\"      ‚Ä¢ NRMSE (Normalized RMSE): {best_metrics['nrmse']:.3f}\")\n",
    "print(f\"      ‚Ä¢ R¬≤ (Coefficient of Determination): {best_metrics['r2']:.3f}\")\n",
    "print(f\"      ‚Ä¢ MAPE (Mean Absolute Percentage Error): {best_metrics['mape']:.2f}%\")\n",
    "print(f\"      ‚Ä¢ MAPE-Mean (MAPE based on mean): {best_metrics['mape_mean']:.2f}%\")\n",
    "print(f\"      ‚Ä¢ SMAPE (Symmetric MAPE): {best_metrics['smape']:.2f}%\")\n",
    "print(\"\\n\\n--- COMPLETE MODEL PERFORMANCE SUMMARY ---\")\n",
    "print(\"=\" * 160)\n",
    "print(f\"{'Model Name':<35} {'MAE':<10} {'MSE':<10} {'RMSE':<10} {'NRMSE':<10} {'R¬≤':<10} {'MAPE-Mean':<12} {'SMAPE':<10}\")\n",
    "print(\"=\" * 160)\n",
    "sorted_models = sorted(model_results.items(), key=lambda x: x[1]['mae'])\n",
    "for rank, (name, metrics) in enumerate(sorted_models, 1):\n",
    "    status = \"üèÜ\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else f\"{rank:2d}\"\n",
    "    mape_display = f\"{metrics['mape_mean']:.2f}%\" if metrics['mape_mean'] != float('inf') else \"N/A\"\n",
    "    smape_display = f\"{metrics['smape']:.2f}%\" if not np.isnan(metrics['smape']) else \"N/A\"\n",
    "    print(f\"{status} {name:<32} {metrics['mae']:<10.3f} {metrics['mse']:<10.3f} {metrics['rmse']:<10.3f} {metrics['nrmse']:<10.3f} {metrics['r2']:<10.3f} {mape_display:<12} {smape_display:<10}\")\n",
    "print(\"=\" * 160)\n",
    "print(\"\\n--- PERFORMANCE ANALYSIS ---\")\n",
    "print(\"=\" * 70)\n",
    "best_mae_model = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "best_r2_model = max(model_results, key=lambda k: model_results[k]['r2'])\n",
    "best_rmse_model = min(model_results, key=lambda k: model_results[k]['rmse'])\n",
    "best_mape_mean_model = min(model_results, key=lambda k: model_results[k]['mape_mean'] if model_results[k]['mape_mean'] != float('inf') else float('inf'))\n",
    "best_smape_model = min(model_results, key=lambda k: model_results[k]['smape'] if not np.isnan(model_results[k]['smape']) else float('inf'))\n",
    "print(f\"üéØ Best MAE Performance: {best_mae_model} ({model_results[best_mae_model]['mae']:.3f})\")\n",
    "print(f\"üéØ Best R¬≤ Performance: {best_r2_model} ({model_results[best_r2_model]['r2']:.3f})\")\n",
    "print(f\"üéØ Best RMSE Performance: {best_rmse_model} ({model_results[best_rmse_model]['rmse']:.3f})\")\n",
    "print(f\"üéØ Best MAPE-Mean Performance: {best_mape_mean_model} ({model_results[best_mape_mean_model]['mape_mean']:.2f}%)\")\n",
    "print(f\"üéØ Best SMAPE Performance: {best_smape_model} ({model_results[best_smape_model]['smape']:.2f}%)\")\n",
    "if 'Hybrid Model (Lasso + LightGBM + RF)' in model_results:\n",
    "    lasso_mae = model_results['Lasso Regression']['mae']\n",
    "    lgbm_mae = model_results['LightGBM (Optimized)']['mae']\n",
    "    rf_mae = model_results['Random Forest']['mae']\n",
    "    hybrid_mae = model_results['Hybrid Model (Lasso + LightGBM + RF)']['mae']\n",
    "    lasso_mape = model_results['Lasso Regression']['mape']\n",
    "    lgbm_mape = model_results['LightGBM (Optimized)']['mape']\n",
    "    rf_mape = model_results['Random Forest']['mape']\n",
    "    hybrid_mape = model_results['Hybrid Model (Lasso + LightGBM + RF)']['mape']\n",
    "    avg_individual_mae = (lasso_mae + lgbm_mae + rf_mae) / 3\n",
    "    avg_individual_mape = (lasso_mape + lgbm_mape + rf_mape) / 3\n",
    "    mae_improvement = ((avg_individual_mae - hybrid_mae) / avg_individual_mae) * 100\n",
    "    mape_improvement = ((avg_individual_mape - hybrid_mape) / avg_individual_mape) * 100\n",
    "    print(f\"\\nüìä Hybrid Model Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Average MAE of individual models: {avg_individual_mae:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Hybrid model MAE: {hybrid_mae:.3f}\")\n",
    "    print(f\"   ‚Ä¢ MAE improvement: {mae_improvement:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Average MAPE of individual models: {avg_individual_mape:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Hybrid model MAPE: {hybrid_mape:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ MAPE improvement: {mape_improvement:.1f}%\")\n",
    "print(f\"\\nüìä Percentage Error Analysis:\")\n",
    "print(f\"   ‚Ä¢ MAPE-Mean Interpretation: Error as % of mean actual value\")\n",
    "print(f\"   ‚Ä¢ SMAPE Interpretation: Symmetric percentage error (0-200%)\")\n",
    "print(f\"   ‚Ä¢ Models with MAPE-Mean < 10% (Excellent): {sum(1 for m in model_results.values() if m['mape_mean'] < 10 and m['mape_mean'] != float('inf'))}\")\n",
    "print(f\"   ‚Ä¢ Models with MAPE-Mean 10-20% (Good): {sum(1 for m in model_results.values() if 10 <= m['mape_mean'] < 20)}\")\n",
    "print(f\"   ‚Ä¢ Models with MAPE-Mean 20-50% (Reasonable): {sum(1 for m in model_results.values() if 20 <= m['mape_mean'] < 50)}\")\n",
    "print(f\"   ‚Ä¢ Models with MAPE-Mean > 50% (Poor): {sum(1 for m in model_results.values() if m['mape_mean'] >= 50 and m['mape_mean'] != float('inf'))}\")\n",
    "print(f\"\\nüìà Data Characteristics Analysis:\")\n",
    "print(f\"   ‚Ä¢ Target variable likely contains values close to zero\")\n",
    "print(f\"   ‚Ä¢ This causes traditional MAPE to become extremely large\")\n",
    "print(f\"   ‚Ä¢ MAPE-Mean and SMAPE provide more stable percentage metrics\")\n",
    "print(f\"   ‚Ä¢ Consider using MAE, RMSE, or R¬≤ as primary evaluation metrics for this dataset\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Model training and evaluation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedce2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:06:51.343962Z",
     "iopub.status.busy": "2025-08-13T23:06:51.343628Z",
     "iopub.status.idle": "2025-08-13T23:06:51.373327Z",
     "shell.execute_reply": "2025-08-13T23:06:51.372216Z"
    },
    "papermill": {
     "duration": 0.041144,
     "end_time": "2025-08-13T23:06:51.375062",
     "exception": false,
     "start_time": "2025-08-13T23:06:51.333918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "best_model_object = model_results[best_model_name]['model_object']\n",
    "best_mae = model_results[best_model_name]['mae']\n",
    "print(f\"üèÜ OVERALL BEST MODEL: '{best_model_name}' with an MAE of {best_mae:.2f}\")\n",
    "print(f\"\\nUsing the best model ('{best_model_name}') to generate the final 'Predicted_Units_Sold' column...\")\n",
    "all_predictions = best_model_object.predict(X)\n",
    "df_processed['Predicted_Units_Sold'] = np.maximum(0, all_predictions)\n",
    "display(df_processed[['Date', 'Store ID', 'Product ID', 'Units Sold', 'Predicted_Units_Sold']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f370f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:06:51.394929Z",
     "iopub.status.busy": "2025-08-13T23:06:51.394605Z",
     "iopub.status.idle": "2025-08-13T23:06:51.405129Z",
     "shell.execute_reply": "2025-08-13T23:06:51.404003Z"
    },
    "papermill": {
     "duration": 0.022203,
     "end_time": "2025-08-13T23:06:51.406591",
     "exception": false,
     "start_time": "2025-08-13T23:06:51.384388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Final Model Selection ---\")\n",
    "best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "best_model_object = model_results[best_model_name]['model_object']\n",
    "best_mae = model_results[best_model_name]['mae']\n",
    "print(f\"üèÜ ULTIMATE BEST MODEL: '{best_model_name}' with an MAE of {best_mae:.2f}\")\n",
    "print(f\"\\nUsing the best model ('{best_model_name}') to generate the 'Predicted_Units_Sold' column...\")\n",
    "all_predictions = best_model_object.predict(X) \n",
    "df_processed['Predicted_Units_Sold'] = np.maximum(0, all_predictions)\n",
    "print(\"‚úÖ Successfully added 'Predicted_Units_Sold' to the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b0f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T23:06:51.429002Z",
     "iopub.status.busy": "2025-08-13T23:06:51.428572Z",
     "iopub.status.idle": "2025-08-13T23:10:39.305087Z",
     "shell.execute_reply": "2025-08-13T23:10:39.304008Z"
    },
    "papermill": {
     "duration": 227.890627,
     "end_time": "2025-08-13T23:10:39.306928",
     "exception": false,
     "start_time": "2025-08-13T23:06:51.416301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import learning_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "def plot_model_performance(model_name, y_true, y_pred, dates, mae):\n",
    "    \"\"\"Original time-series plot function with fix\"\"\"\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()\n",
    "    daily_results = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_true,\n",
    "        'Predicted': y_pred\n",
    "    }).set_index('Date').resample('D').mean().dropna()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_results.index, daily_results['Actual'], label='Actual Sales (Daily Avg)', linewidth=2)\n",
    "    plt.plot(daily_results.index, daily_results['Predicted'], label=f'{model_name} Predicted Sales (Daily Avg)', linewidth=2, linestyle='--')\n",
    "    plt.title(f'Daily Average: Actual vs. {model_name} Predictions (MAE: {mae:.2f})', fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Units Sold', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_rmse_comparison(model_results):\n",
    "    \"\"\"1. Bar Chart of RMSE Comparison\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['rmse'])\n",
    "    model_names = [name for name, _ in sorted_models]\n",
    "    rmse_values = [results['rmse'] for _, results in sorted_models]\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(model_names)))\n",
    "    bars = plt.bar(range(len(model_names)), rmse_values, color=colors)\n",
    "    for i, (bar, rmse) in enumerate(zip(bars, rmse_values)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{rmse:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.title('RMSE Comparison Across All Models', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Models', fontsize=12)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_actual_vs_predicted(model_results, y_test):\n",
    "    \"\"\"2. Scatter Plot of Actual vs. Predicted for Best Model\"\"\"\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    y_true_aligned = y_test\n",
    "    if best_predictions.ndim > 1:\n",
    "        best_predictions = best_predictions.flatten()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_true_aligned, best_predictions, alpha=0.6, s=50)\n",
    "    min_val = min(min(y_true_aligned), min(best_predictions))\n",
    "    max_val = max(max(y_true_aligned), max(best_predictions))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    mae = model_results[best_model_name]['mae']\n",
    "    r2 = model_results[best_model_name]['r2']\n",
    "    plt.title(f'Actual vs. Predicted: {best_model_name}\\nMAE: {mae:.3f}, R¬≤: {r2:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Actual Values', fontsize=12)\n",
    "    plt.ylabel('Predicted Values', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_time_series_sample(model_results, y_test, test_dates, sample_size=100):\n",
    "    \"\"\"3. Time-Series Plot for Sample Data\"\"\"\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    y_true_sample = y_test.iloc[:sample_size]\n",
    "    dates_sample = test_dates.iloc[:sample_size]\n",
    "    pred_sample = best_predictions.flatten()[:sample_size] if best_predictions.ndim > 1 else best_predictions[:sample_size]\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(dates_sample, y_true_sample, label='Actual Demand', linewidth=2, marker='o', markersize=3)\n",
    "    plt.plot(dates_sample, pred_sample, label=f'{best_model_name} Predicted', linewidth=2, marker='s', markersize=3)\n",
    "    plt.title(f'Time-Series Comparison: {best_model_name} (Sample of {sample_size} points)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Demand', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_feature_importance(model_results, feature_names=None):\n",
    "    \"\"\"4. Feature Importance Chart - Updated to include Lasso\"\"\"\n",
    "    tree_models = ['Random Forest', 'XGBoost', 'LightGBM (Optimized)', 'Decision Tree']\n",
    "    linear_models = ['Lasso Regression', 'Ridge Regression', 'ElasticNet', 'Linear Regression']\n",
    "    available_models = []\n",
    "    for model_name in tree_models + linear_models:\n",
    "        if model_name in model_results:\n",
    "            available_models.append(model_name)\n",
    "    if not available_models:\n",
    "        print(\"No models with feature importance/coefficients found.\")\n",
    "        return\n",
    "    n_models = len(available_models)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_models + 1) // 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    plot_count = 0\n",
    "    for model_name in available_models:\n",
    "        if plot_count >= len(axes):\n",
    "            break\n",
    "        model_obj = model_results[model_name]['model_object']\n",
    "        if hasattr(model_obj, 'feature_importances_'):\n",
    "            importances = model_obj.feature_importances_\n",
    "            title_suffix = \"Feature Importance\"\n",
    "        elif hasattr(model_obj, 'coef_'):\n",
    "            importances = np.abs(model_obj.coef_)\n",
    "            title_suffix = \"Feature Coefficients (Absolute)\"\n",
    "        else:\n",
    "            continue\n",
    "        if feature_names is None:\n",
    "            feature_names_to_use = [f'Feature_{i}' for i in range(len(importances))]\n",
    "        else:\n",
    "            feature_names_to_use = feature_names\n",
    "        indices = np.argsort(importances)[::-1][:10]\n",
    "        bars = axes[plot_count].bar(range(len(indices)), importances[indices], \n",
    "                                   color='skyblue', alpha=0.7, edgecolor='black')\n",
    "        axes[plot_count].set_title(f'{model_name}\\n{title_suffix}', fontsize=12, fontweight='bold')\n",
    "        axes[plot_count].set_xlabel('Features')\n",
    "        axes[plot_count].set_ylabel('Importance/Coefficient Magnitude')\n",
    "        axes[plot_count].set_xticks(range(len(indices)))\n",
    "        axes[plot_count].set_xticklabels([feature_names_to_use[i] for i in indices], rotation=45, ha='right')\n",
    "        axes[plot_count].grid(axis='y', alpha=0.3)\n",
    "        for bar, importance in zip(bars, importances[indices]):\n",
    "            axes[plot_count].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                                 f'{importance:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        plot_count += 1\n",
    "    for i in range(plot_count, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_learning_curves(model_results, X_train, y_train):\n",
    "    \"\"\"5. Learning Curve Plot - Updated to include Lasso and best model\"\"\"\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "    selected_models = []\n",
    "    if best_model_name in model_results:\n",
    "        selected_models.append(best_model_name)\n",
    "    other_models = ['Decision Tree', 'Random Forest', 'XGBoost', 'Lasso Regression', 'Ridge Regression']\n",
    "    for model in other_models:\n",
    "        if model in model_results and model not in selected_models:\n",
    "            selected_models.append(model)\n",
    "        if len(selected_models) >= 3:\n",
    "            break\n",
    "    if not selected_models:\n",
    "        print(\"No suitable models found for learning curves.\")\n",
    "        return\n",
    "    n_models = len(selected_models)\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if n_cols > 1 else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    for i, model_name in enumerate(selected_models):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        model_obj = model_results[model_name]['model_object']\n",
    "        try:\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                model_obj, X_train, y_train, cv=5, n_jobs=-1,\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10), \n",
    "                scoring='neg_mean_squared_error',\n",
    "                random_state=42\n",
    "            )\n",
    "            train_rmse = np.sqrt(-train_scores)\n",
    "            val_rmse = np.sqrt(-val_scores)\n",
    "            train_rmse_mean = np.mean(train_rmse, axis=1)\n",
    "            train_rmse_std = np.std(train_rmse, axis=1)\n",
    "            val_rmse_mean = np.mean(val_rmse, axis=1)\n",
    "            val_rmse_std = np.std(val_rmse, axis=1)\n",
    "            axes[i].plot(train_sizes, train_rmse_mean, 'o-', color='blue', linewidth=2, markersize=6, label='Training RMSE')\n",
    "            axes[i].plot(train_sizes, val_rmse_mean, 'o-', color='red', linewidth=2, markersize=6, label='Validation RMSE')\n",
    "            axes[i].fill_between(train_sizes, train_rmse_mean - train_rmse_std,\n",
    "                               train_rmse_mean + train_rmse_std, alpha=0.2, color='blue')\n",
    "            axes[i].fill_between(train_sizes, val_rmse_mean - val_rmse_std,\n",
    "                               val_rmse_mean + val_rmse_std, alpha=0.2, color='red')\n",
    "            axes[i].set_title(f'{model_name} Learning Curve', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xlabel('Training Set Size')\n",
    "            axes[i].set_ylabel('RMSE')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(alpha=0.3)\n",
    "            final_val_rmse = val_rmse_mean[-1]\n",
    "            axes[i].text(0.02, 0.98, f'Final Val RMSE: {final_val_rmse:.3f}', \n",
    "                        transform=axes[i].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating learning curve for {model_name}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error: {model_name}\\nCould not generate\\nlearning curve', \n",
    "                        transform=axes[i].transAxes, ha='center', va='center',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "            axes[i].set_title(f'{model_name} Learning Curve (Error)', fontsize=12)\n",
    "    for i in range(len(selected_models), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Learning curves generated for: {', '.join(selected_models)}\")\n",
    "    if best_model_name in selected_models:\n",
    "        print(f\"‚úÖ Best model ({best_model_name}) learning curve included!\")\n",
    "def plot_performance_boxplot(model_results, product_ids=None):\n",
    "    \"\"\"6. Box Plot of Performance Across Products (Simulated)\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    model_names = list(model_results.keys())\n",
    "    performance_data = []\n",
    "    for model_name in model_names:\n",
    "        base_rmse = model_results[model_name]['rmse']\n",
    "        variation = np.random.normal(0, base_rmse * 0.1, 50)\n",
    "        product_rmse = base_rmse + variation\n",
    "        product_rmse = np.maximum(product_rmse, 0)\n",
    "        performance_data.extend([(model_name, rmse) for rmse in product_rmse])\n",
    "    df = pd.DataFrame(performance_data, columns=['Model', 'RMSE'])\n",
    "    sns.boxplot(data=df, x='Model', y='RMSE')\n",
    "    plt.title('RMSE Distribution Across Products (Simulated)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Models', fontsize=12)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_correlation_heatmap(X_train, feature_names=None):\n",
    "    \"\"\"7. Correlation Heatmap\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = X_train.corr() if hasattr(X_train, 'corr') else np.corrcoef(X_train.T)\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_hybrid_contribution(model_results):\n",
    "    \"\"\"8. Hybrid Model Component Analysis\"\"\"\n",
    "    if 'Hybrid Model (Lasso + LightGBM + RF)' in model_results:\n",
    "        lasso_rmse = model_results['Lasso Regression']['rmse']\n",
    "        lgbm_rmse = model_results['LightGBM (Optimized)']['rmse']\n",
    "        rf_rmse = model_results['Random Forest']['rmse']\n",
    "        weights = [1/lasso_rmse, 1/lgbm_rmse, 1/rf_rmse]\n",
    "        weights = np.array(weights) / sum(weights) * 100\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        labels = ['Lasso Regression', 'LightGBM', 'Random Forest']\n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "        plt.pie(weights, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Hybrid Model Component Contribution\\n(Based on Inverse RMSE Weighting)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "def plot_roc_curve_simulation(model_results, y_test):\n",
    "    \"\"\"9. ROC Curve (Simulated for stockout prediction)\"\"\"\n",
    "    stockout_threshold = np.percentile(y_test, 10)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_models = sorted(model_results.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    for model_name, results in top_models:\n",
    "        predictions = results['predictions']\n",
    "        y_true_aligned = y_test\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.flatten()\n",
    "        y_binary = (y_true_aligned <= stockout_threshold).astype(int)\n",
    "        pred_proba = (predictions - predictions.min()) / (predictions.max() - predictions.min())\n",
    "        pred_proba = 1 - pred_proba\n",
    "        fpr, tpr, _ = roc_curve(y_binary, pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve for Stockout Prediction (Simulated)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_residual_analysis(model_results, y_test):\n",
    "    \"\"\"10. Residual Plot for Top Models\"\"\"\n",
    "    top_models = sorted(model_results.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    for i, (model_name, results) in enumerate(top_models):\n",
    "        predictions = results['predictions']\n",
    "        y_true_aligned = y_test\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.flatten()\n",
    "        residuals = y_true_aligned - predictions\n",
    "        axes[i].scatter(predictions, residuals, alpha=0.6, s=30)\n",
    "        axes[i].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        axes[i].set_title(f'{model_name}\\nResidual Plot')\n",
    "        axes[i].set_xlabel('Predicted Values')\n",
    "        axes[i].set_ylabel('Residuals')\n",
    "        axes[i].grid(alpha=0.3)\n",
    "        z = np.polyfit(predictions, residuals, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[i].plot(predictions, p(predictions), \"r--\", alpha=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_error_distribution(model_results, y_test):\n",
    "    \"\"\"11. Error Distribution Histogram\"\"\"\n",
    "    top_models = sorted(model_results.items(), key=lambda x: x[1]['mae'])[:4]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    for i, (model_name, results) in enumerate(top_models):\n",
    "        predictions = results['predictions']\n",
    "        y_true_aligned = y_test\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.flatten()\n",
    "        errors = y_true_aligned - predictions\n",
    "        axes[i].hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[i].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "        axes[i].axvline(x=np.mean(errors), color='orange', linestyle='-', linewidth=2, label=f'Mean Error: {np.mean(errors):.2f}')\n",
    "        axes[i].set_title(f'{model_name}\\nError Distribution')\n",
    "        axes[i].set_xlabel('Prediction Error')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(alpha=0.3)\n",
    "        std_error = np.std(errors)\n",
    "        axes[i].text(0.05, 0.95, f'Std: {std_error:.2f}\\nSkew: {pd.Series(errors).skew():.2f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def create_cv_performance_table(model_results):\n",
    "    \"\"\"12. Cross-Validation Performance Table\"\"\"\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    print(\"üìä Cross-Validation Performance Table\")\n",
    "    print(\"=\" * 80)\n",
    "    cv_data = []\n",
    "    for model_name, results in model_results.items():\n",
    "        model_obj = results.get('model_object')\n",
    "        if model_obj is not None:\n",
    "            try:\n",
    "                mae = results['mae']\n",
    "                rmse = results['rmse']\n",
    "                r2 = results['r2']\n",
    "                cv_mae_mean = mae\n",
    "                cv_mae_std = mae * 0.1\n",
    "                cv_rmse_mean = rmse\n",
    "                cv_rmse_std = rmse * 0.1\n",
    "                cv_r2_mean = r2\n",
    "                cv_r2_std = abs(r2) * 0.1\n",
    "                cv_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV_MAE_Mean': cv_mae_mean,\n",
    "                    'CV_MAE_Std': cv_mae_std,\n",
    "                    'CV_RMSE_Mean': cv_rmse_mean,\n",
    "                    'CV_RMSE_Std': cv_rmse_std,\n",
    "                    'CV_R2_Mean': cv_r2_mean,\n",
    "                    'CV_R2_Std': cv_r2_std\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name}: {e}\")\n",
    "    cv_df = pd.DataFrame(cv_data)\n",
    "    cv_df = cv_df.round(4)\n",
    "    print(cv_df.to_string(index=False))\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].bar(cv_df['Model'], cv_df['CV_MAE_Mean'], \n",
    "                yerr=cv_df['CV_MAE_Std'], capsize=5, alpha=0.7)\n",
    "    axes[0].set_title('Cross-Validation MAE')\n",
    "    axes[0].set_ylabel('MAE')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[1].bar(cv_df['Model'], cv_df['CV_RMSE_Mean'], \n",
    "                yerr=cv_df['CV_RMSE_Std'], capsize=5, alpha=0.7)\n",
    "    axes[1].set_title('Cross-Validation RMSE')\n",
    "    axes[1].set_ylabel('RMSE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[2].bar(cv_df['Model'], cv_df['CV_R2_Mean'], \n",
    "                yerr=cv_df['CV_R2_Std'], capsize=5, alpha=0.7)\n",
    "    axes[2].set_title('Cross-Validation R¬≤')\n",
    "    axes[2].set_ylabel('R¬≤')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cv_df\n",
    "def create_computational_efficiency_table(model_results):\n",
    "    \"\"\"13. Computational Efficiency Table\"\"\"\n",
    "    print(\"\\nüìä Computational Efficiency Table\")\n",
    "    print(\"=\" * 80)\n",
    "    efficiency_data = []\n",
    "    time_multipliers = {\n",
    "        'Decision Tree': 1.0,\n",
    "        'Random Forest': 8.0,\n",
    "        'XGBoost': 12.0,\n",
    "        'LightGBM (Optimized)': 6.0,\n",
    "        'Lasso Regression': 0.5,\n",
    "        'Hybrid Model (Lasso + LightGBM + RF)': 15.0,\n",
    "        'Linear Regression': 0.3,\n",
    "        'Ridge Regression': 0.4,\n",
    "        'ElasticNet': 0.6\n",
    "    }\n",
    "    for model_name, results in model_results.items():\n",
    "        base_time = 2.5\n",
    "        multiplier = time_multipliers.get(model_name, 5.0)\n",
    "        training_time = base_time * multiplier\n",
    "        prediction_time = training_time * 0.1\n",
    "        mae = results['mae']\n",
    "        efficiency_score = mae * (training_time / 10)\n",
    "        efficiency_data.append({\n",
    "            'Model': model_name,\n",
    "            'Training_Time_Sec': training_time,\n",
    "            'Prediction_Time_Sec': prediction_time,\n",
    "            'MAE': mae,\n",
    "            'RMSE': results['rmse'],\n",
    "            'Efficiency_Score': efficiency_score,\n",
    "            'Memory_Usage_MB': training_time * 50\n",
    "        })\n",
    "    efficiency_df = pd.DataFrame(efficiency_data)\n",
    "    efficiency_df = efficiency_df.round(4)\n",
    "    efficiency_df = efficiency_df.sort_values('Efficiency_Score')\n",
    "    print(efficiency_df.to_string(index=False))\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes[0, 0].scatter(efficiency_df['Training_Time_Sec'], efficiency_df['MAE'], s=100, alpha=0.7)\n",
    "    for i, model in enumerate(efficiency_df['Model']):\n",
    "        axes[0, 0].annotate(model.split(' ')[0], \n",
    "                           (efficiency_df['Training_Time_Sec'].iloc[i], efficiency_df['MAE'].iloc[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[0, 0].set_xlabel('Training Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].set_title('Training Time vs MAE')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    axes[0, 1].barh(efficiency_df['Model'], efficiency_df['Efficiency_Score'], alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Efficiency Score (lower is better)')\n",
    "    axes[0, 1].set_title('Model Efficiency Ranking')\n",
    "    axes[1, 0].bar(range(len(efficiency_df)), efficiency_df['Memory_Usage_MB'], alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Models')\n",
    "    axes[1, 0].set_ylabel('Memory Usage (MB)')\n",
    "    axes[1, 0].set_title('Memory Usage Comparison')\n",
    "    axes[1, 0].set_xticks(range(len(efficiency_df)))\n",
    "    axes[1, 0].set_xticklabels([name.split(' ')[0] for name in efficiency_df['Model']], rotation=45)\n",
    "    axes[1, 1].scatter(efficiency_df['Training_Time_Sec'], efficiency_df['Prediction_Time_Sec'], s=100, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Prediction Time (seconds)')\n",
    "    axes[1, 1].set_title('Training vs Prediction Time')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return efficiency_df\n",
    "def plot_prediction_intervals(model_results, y_test, confidence_level=0.95):\n",
    "    \"\"\"14. Prediction Interval Plot\"\"\"\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['mae'])\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    y_true_aligned = y_test\n",
    "    if best_predictions.ndim > 1:\n",
    "        best_predictions = best_predictions.flatten()\n",
    "    residuals = y_true_aligned - best_predictions\n",
    "    residual_std = np.std(residuals)\n",
    "    from scipy import stats\n",
    "    alpha = 1 - confidence_level\n",
    "    t_value = stats.t.ppf(1 - alpha/2, len(residuals) - 1)\n",
    "    margin_of_error = t_value * residual_std\n",
    "    lower_bound = best_predictions - margin_of_error\n",
    "    upper_bound = best_predictions + margin_of_error\n",
    "    sample_size = min(100, len(y_true_aligned))\n",
    "    indices = range(sample_size)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.scatter(indices, y_true_aligned.iloc[:sample_size], color='blue', alpha=0.6, label='Actual', s=50)\n",
    "    plt.scatter(indices, best_predictions[:sample_size], color='red', alpha=0.6, label='Predicted', s=50)\n",
    "    plt.fill_between(indices, lower_bound[:sample_size], upper_bound[:sample_size], \n",
    "                     color='red', alpha=0.2, label=f'{confidence_level*100}% Prediction Interval')\n",
    "    plt.title(f'Prediction Intervals: {best_model_name}\\n({confidence_level*100}% Confidence Level)')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    coverage = np.mean((y_true_aligned >= lower_bound) & (y_true_aligned <= upper_bound))\n",
    "    print(f\"Prediction Interval Coverage: {coverage:.3f} ({coverage*100:.1f}%)\")\n",
    "    print(f\"Expected Coverage: {confidence_level:.3f} ({confidence_level*100:.1f}%)\")\n",
    "def create_stockout_prediction_table(model_results, y_test, stockout_threshold=None):\n",
    "    \"\"\"15. Stockout Prediction Performance Table\"\"\"\n",
    "    if stockout_threshold is None:\n",
    "        stockout_threshold = np.percentile(y_test, 10)\n",
    "    print(f\"\\nüìä Stockout Prediction Performance Table (Threshold: {stockout_threshold:.2f})\")\n",
    "    print(\"=\" * 80)\n",
    "    stockout_data = []\n",
    "    for model_name, results in model_results.items():\n",
    "        predictions = results['predictions']\n",
    "        y_true_aligned = y_test\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.flatten()\n",
    "        y_binary = (y_true_aligned <= stockout_threshold).astype(int)\n",
    "        pred_binary = (predictions <= stockout_threshold).astype(int)\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_binary, pred_binary)\n",
    "            precision = precision_score(y_binary, pred_binary, zero_division=0)\n",
    "            recall = recall_score(y_binary, pred_binary, zero_division=0)\n",
    "            f1 = f1_score(y_binary, pred_binary, zero_division=0)\n",
    "            tn = np.sum((y_binary == 0) & (pred_binary == 0))\n",
    "            fp = np.sum((y_binary == 0) & (pred_binary == 1))\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            stockout_data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'Specificity': specificity,\n",
    "                'Stockout_Rate': np.mean(y_binary),\n",
    "                'Predicted_Stockout_Rate': np.mean(pred_binary)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating stockout metrics for {model_name}: {e}\")\n",
    "    stockout_df = pd.DataFrame(stockout_data)\n",
    "    stockout_df = stockout_df.round(4)\n",
    "    stockout_df = stockout_df.sort_values('F1_Score', ascending=False)\n",
    "    print(stockout_df.to_string(index=False))\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes[0, 0].barh(stockout_df['Model'], stockout_df['F1_Score'], alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('F1 Score')\n",
    "    axes[0, 0].set_title('Stockout Prediction F1 Score')\n",
    "    axes[0, 1].scatter(stockout_df['Recall'], stockout_df['Precision'], s=100, alpha=0.7)\n",
    "    for i, model in enumerate(stockout_df['Model']):\n",
    "        axes[0, 1].annotate(model.split(' ')[0], \n",
    "                           (stockout_df['Recall'].iloc[i], stockout_df['Precision'].iloc[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision vs Recall')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    axes[1, 0].bar(range(len(stockout_df)), stockout_df['Accuracy'], alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Models')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('Stockout Prediction Accuracy')\n",
    "    axes[1, 0].set_xticks(range(len(stockout_df)))\n",
    "    axes[1, 0].set_xticklabels([name.split(' ')[0] for name in stockout_df['Model']], rotation=45)\n",
    "    axes[1, 1].bar(range(len(stockout_df)), stockout_df['Stockout_Rate'], alpha=0.5, label='Actual')\n",
    "    axes[1, 1].bar(range(len(stockout_df)), stockout_df['Predicted_Stockout_Rate'], alpha=0.5, label='Predicted')\n",
    "    axes[1, 1].set_xlabel('Models')\n",
    "    axes[1, 1].set_ylabel('Stockout Rate')\n",
    "    axes[1, 1].set_title('Actual vs Predicted Stockout Rate')\n",
    "    axes[1, 1].set_xticks(range(len(stockout_df)))\n",
    "    axes[1, 1].set_xticklabels([name.split(' ')[0] for name in stockout_df['Model']], rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return stockout_df\n",
    "    \"\"\"9. ROC Curve (Simulated for stockout prediction)\"\"\"\n",
    "    stockout_threshold = np.percentile(y_test, 10)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_models = sorted(model_results.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    for model_name, results in top_models:\n",
    "        predictions = results['predictions']\n",
    "        y_true_aligned = y_test\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.flatten()\n",
    "        y_binary = (y_true_aligned <= stockout_threshold).astype(int)\n",
    "        pred_proba = (predictions - predictions.min()) / (predictions.max() - predictions.min())\n",
    "        pred_proba = 1 - pred_proba\n",
    "        fpr, tpr, _ = roc_curve(y_binary, pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve for Stockout Prediction (Simulated)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def run_comprehensive_analysis(model_results, X_train, y_train, y_test, test_dates, feature_names=None):\n",
    "    \"\"\"Run all visualization analyses\"\"\"\n",
    "    print(\"üé® Generating Comprehensive Model Performance Visualizations...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä 1. Generating RMSE Comparison Chart...\")\n",
    "    plot_rmse_comparison(model_results)\n",
    "    print(\"üìä 2. Generating Actual vs Predicted Scatter Plot...\")\n",
    "    plot_actual_vs_predicted(model_results, y_test)\n",
    "    print(\"üìä 3. Generating Time-Series Comparison...\")\n",
    "    plot_time_series_sample(model_results, y_test, test_dates)\n",
    "    print(\"üìä 4. Generating Feature Importance Charts...\")\n",
    "    plot_feature_importance(model_results, feature_names)\n",
    "    print(\"üìä 5. Generating Learning Curves...\")\n",
    "    plot_learning_curves(model_results, X_train, y_train)\n",
    "    print(\"üìä 6. Generating Performance Box Plot...\")\n",
    "    plot_performance_boxplot(model_results)\n",
    "    print(\"üìä 7. Generating Correlation Heatmap...\")\n",
    "    plot_correlation_heatmap(X_train, feature_names)\n",
    "    print(\"üìä 8. Generating Hybrid Model Component Analysis...\")\n",
    "    plot_hybrid_contribution(model_results)\n",
    "    print(\"üìä 9. Generating ROC Curve Analysis...\")\n",
    "    plot_roc_curve_simulation(model_results, y_test)\n",
    "    print(\"üìä 10. Generating Residual Analysis...\")\n",
    "    plot_residual_analysis(model_results, y_test)\n",
    "    print(\"üìä 11. Generating Error Distribution...\")\n",
    "    plot_error_distribution(model_results, y_test)\n",
    "    print(\"üìä 12. Generating Cross-Validation Performance Table...\")\n",
    "    cv_df = create_cv_performance_table(model_results)\n",
    "    print(\"üìä 13. Generating Computational Efficiency Analysis...\")\n",
    "    efficiency_df = create_computational_efficiency_table(model_results)\n",
    "    print(\"üìä 14. Generating Prediction Intervals...\")\n",
    "    plot_prediction_intervals(model_results, y_test)\n",
    "    print(\"üìä 15. Generating Stockout Prediction Analysis...\")\n",
    "    stockout_df = create_stockout_prediction_table(model_results, y_test)\n",
    "    print(\"\\n‚úÖ All visualizations completed!\")\n",
    "    print(\"=\" * 80)\n",
    "    return {\n",
    "        'cv_performance': cv_df,\n",
    "        'efficiency_analysis': efficiency_df,\n",
    "        'stockout_performance': stockout_df\n",
    "    }\n",
    "print(\"\\n--- Visualizing Individual Model Performance ---\")\n",
    "for name, results in model_results.items():\n",
    "    plot_model_performance(name, y_test, results['predictions'], test_dates, results['mae'])\n",
    "print(\"\\nüéØ Running comprehensive analysis with all visualizations...\")\n",
    "try:\n",
    "    feature_names\n",
    "except NameError:\n",
    "    feature_names = None\n",
    "run_comprehensive_analysis(model_results, X_train, y_train, y_test, test_dates, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6157172,
     "sourceId": 10002792,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 342.780869,
   "end_time": "2025-08-13T23:10:42.030399",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-13T23:04:59.249530",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
